The codebook size and expressive power of the compressors are linked.  If the compressor is unable to predict a sequence based on context where each subsequent byte has entropy less than (calculated from logits) the byte that preceeded it then it will be unable to meaningfully compress the byte sequence.  If we arent able to get longer sequences of approximately monotonically decreasing entropy then we will never utilize a codebook of sufficient size.  These limits can probably be empirically explored by setting a codebook to be far too large and training a compressor until the vq codebook reset sizes stabilize, whatever the codebook size is at that time is likely an appropriate codebook size for a compressor of that "expressive power"  This may also be a neat way to measure a new type of expressive power.